{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:00:19.658779Z","iopub.status.busy":"2023-04-19T00:00:19.658361Z","iopub.status.idle":"2023-04-19T00:00:19.671100Z","shell.execute_reply":"2023-04-19T00:00:19.669999Z","shell.execute_reply.started":"2023-04-19T00:00:19.658696Z"},"trusted":true},"outputs":[],"source":["ratio_ViT_L_16          = 0.3\n","ratio_CLIP_Interrogator = 0.3\n","ratio_myclip            = 0.3"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:00:19.673711Z","iopub.status.busy":"2023-04-19T00:00:19.673076Z","iopub.status.idle":"2023-04-19T00:00:35.923064Z","shell.execute_reply":"2023-04-19T00:00:35.921901Z","shell.execute_reply.started":"2023-04-19T00:00:19.673663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# Using the pre compiled wheel since we don't have internet on submission\n","!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:00:35.927136Z","iopub.status.busy":"2023-04-19T00:00:35.926092Z","iopub.status.idle":"2023-04-19T00:00:37.082239Z","shell.execute_reply":"2023-04-19T00:00:37.081123Z","shell.execute_reply.started":"2023-04-19T00:00:35.927099Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import glob\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","import torch\n","\n","import gc\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:01:09.654023Z","iopub.status.busy":"2023-04-19T00:01:09.653537Z","iopub.status.idle":"2023-04-19T00:01:12.777362Z","shell.execute_reply":"2023-04-19T00:01:12.776385Z","shell.execute_reply.started":"2023-04-19T00:01:09.653984Z"},"trusted":true},"outputs":[],"source":["sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('../input/stable-diffusion-image-to-prompts/')\n","st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:01:32.095934Z","iopub.status.busy":"2023-04-19T00:01:32.095357Z","iopub.status.idle":"2023-04-19T00:01:32.102897Z","shell.execute_reply":"2023-04-19T00:01:32.101758Z","shell.execute_reply.started":"2023-04-19T00:01:32.095897Z"},"trusted":true},"outputs":[],"source":["wheels_path = \"/kaggle/input/clip-interrogator-wheels-x\"\n","clip_interrogator_whl_path = f\"{wheels_path}/clip_interrogator-0.4.3-py3-none-any.whl\""]},{"cell_type":"code","execution_count":12,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-04-19T00:01:32.104828Z","iopub.status.busy":"2023-04-19T00:01:32.104374Z","iopub.status.idle":"2023-04-19T00:01:58.401427Z","shell.execute_reply":"2023-04-19T00:01:58.400160Z","shell.execute_reply.started":"2023-04-19T00:01:32.104788Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\n","beatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\n","tfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\n","tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n","tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.8.0 which is incompatible.\n","tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\n","tensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\n","tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n","tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\n","tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\n","ortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.20.3 which is incompatible.\n","onnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\n","nnabla 1.33.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\n","gcsfs 2022.5.0 requires fsspec==2022.5.0, but you have fsspec 2023.1.0 which is incompatible.\n","apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n","allennlp 2.10.1 requires fairscale==0.4.6, but you have fairscale 0.4.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --no-index --find-links $wheels_path $clip_interrogator_whl_path -q"]},{"cell_type":"code","execution_count":14,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2023-04-19T00:02:02.203506Z","iopub.status.busy":"2023-04-19T00:02:02.203074Z","iopub.status.idle":"2023-04-19T00:02:03.169513Z","shell.execute_reply":"2023-04-19T00:02:03.168439Z","shell.execute_reply.started":"2023-04-19T00:02:02.203463Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["import inspect\n","import importlib\n","\n","from blip.models import blip\n","from clip_interrogator import clip_interrogator"]},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-04-19T00:02:03.172480Z","iopub.status.busy":"2023-04-19T00:02:03.171237Z","iopub.status.idle":"2023-04-19T00:02:03.187526Z","shell.execute_reply":"2023-04-19T00:02:03.186536Z","shell.execute_reply.started":"2023-04-19T00:02:03.172435Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<module 'blip.models.blip' from '/opt/conda/lib/python3.7/site-packages/blip/models/blip.py'>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# replace tokenizer path to prevent downloading\n","blip_path = inspect.getfile(blip)\n","\n","fin = open(blip_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n","    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\")\n","    \n","fin.close()\n","\n","fin = open(blip_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","# reload module\n","importlib.reload(blip)"]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-04-19T00:02:03.190048Z","iopub.status.busy":"2023-04-19T00:02:03.188915Z","iopub.status.idle":"2023-04-19T00:02:03.209445Z","shell.execute_reply":"2023-04-19T00:02:03.208290Z","shell.execute_reply.started":"2023-04-19T00:02:03.190010Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<module 'clip_interrogator.clip_interrogator' from '/opt/conda/lib/python3.7/site-packages/clip_interrogator/clip_interrogator.py'>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# fix clip_interrogator bug\n","clip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n","\n","fin = open(clip_interrogator_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    'open_clip.get_tokenizer(clip_model_name)', \n","    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])')\n","    \n","fin.close()\n","\n","fin = open(clip_interrogator_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","# reload module\n","importlib.reload(clip_interrogator)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:03.211578Z","iopub.status.busy":"2023-04-19T00:02:03.211206Z","iopub.status.idle":"2023-04-19T00:02:03.219020Z","shell.execute_reply":"2023-04-19T00:02:03.218037Z","shell.execute_reply.started":"2023-04-19T00:02:03.211542Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt \n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import open_clip\n","from sentence_transformers import SentenceTransformer, models"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:03.221287Z","iopub.status.busy":"2023-04-19T00:02:03.220922Z","iopub.status.idle":"2023-04-19T00:02:03.229496Z","shell.execute_reply":"2023-04-19T00:02:03.228516Z","shell.execute_reply.started":"2023-04-19T00:02:03.221252Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    device = \"cuda\"\n","    seed = 42\n","    embedding_length = 384\n","    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n","    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n","    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n","    clip_model_name = \"ViT-H-14\"\n","    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n","    cache_path = \"/kaggle/input/clip-interrogator-models-x\""]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:03.231417Z","iopub.status.busy":"2023-04-19T00:02:03.230948Z","iopub.status.idle":"2023-04-19T00:02:03.249142Z","shell.execute_reply":"2023-04-19T00:02:03.248052Z","shell.execute_reply.started":"2023-04-19T00:02:03.231379Z"},"trusted":true},"outputs":[],"source":["images = os.listdir(comp_path / 'images')\n","imgIds = [i.split('.')[0] for i in images]\n","\n","eIds = list(range(CFG.embedding_length))\n","\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, CFG.embedding_length),\n","        np.tile(range(CFG.embedding_length), len(imgIds)))]"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:03.250788Z","iopub.status.busy":"2023-04-19T00:02:03.250455Z","iopub.status.idle":"2023-04-19T00:02:03.563685Z","shell.execute_reply":"2023-04-19T00:02:03.562730Z","shell.execute_reply.started":"2023-04-19T00:02:03.250762Z"},"trusted":true},"outputs":[],"source":["st_model = SentenceTransformer(CFG.sentence_model_path)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:03.565766Z","iopub.status.busy":"2023-04-19T00:02:03.565387Z","iopub.status.idle":"2023-04-19T00:02:03.573194Z","shell.execute_reply":"2023-04-19T00:02:03.572179Z","shell.execute_reply.started":"2023-04-19T00:02:03.565728Z"},"trusted":true},"outputs":[],"source":["model_config = clip_interrogator.Config(clip_model_name=CFG.ci_clip_model_name)\n","model_config.cache_path = CFG.cache_path"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:03.575158Z","iopub.status.busy":"2023-04-19T00:02:03.574857Z","iopub.status.idle":"2023-04-19T00:02:35.270724Z","shell.execute_reply":"2023-04-19T00:02:35.269670Z","shell.execute_reply.started":"2023-04-19T00:02:03.575133Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["load checkpoint from /kaggle/input/clip-interrogator-models-x/model_large_caption.pth\n"]}],"source":["configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\n","med_config = os.path.join(configs_path, 'med_config.json')\n","blip_model = blip.blip_decoder(\n","    pretrained=CFG.blip_model_path,\n","    image_size=model_config.blip_image_eval_size, \n","    vit=model_config.blip_model_type, \n","    med_config=med_config)\n","    \n","blip_model = blip_model.to(device).eval()\n","model_config.blip_model = blip_model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:02:35.272892Z","iopub.status.busy":"2023-04-19T00:02:35.272285Z","iopub.status.idle":"2023-04-19T00:03:27.848982Z","shell.execute_reply":"2023-04-19T00:03:27.847880Z","shell.execute_reply.started":"2023-04-19T00:02:35.272826Z"},"trusted":true},"outputs":[],"source":["clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\n","open_clip.load_checkpoint(clip_model, CFG.clip_model_path)\n","clip_model.to(device).eval()\n","model_config.clip_model = clip_model"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:27.851397Z","iopub.status.busy":"2023-04-19T00:03:27.850773Z","iopub.status.idle":"2023-04-19T00:03:27.857928Z","shell.execute_reply":"2023-04-19T00:03:27.856719Z","shell.execute_reply.started":"2023-04-19T00:03:27.851359Z"},"trusted":true},"outputs":[],"source":["clip_preprocess = open_clip.image_transform(\n","    clip_model.visual.image_size,\n","    is_train = False,\n","    mean = getattr(clip_model.visual, 'image_mean', None),\n","    std = getattr(clip_model.visual, 'image_std', None))\n","\n","model_config.clip_preprocess = clip_preprocess"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:27.859925Z","iopub.status.busy":"2023-04-19T00:03:27.859550Z","iopub.status.idle":"2023-04-19T00:03:30.687133Z","shell.execute_reply":"2023-04-19T00:03:30.686047Z","shell.execute_reply.started":"2023-04-19T00:03:27.859888Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded CLIP model and data in 2.82 seconds.\n"]}],"source":["ci = clip_interrogator.Interrogator(model_config)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:30.690135Z","iopub.status.busy":"2023-04-19T00:03:30.688582Z","iopub.status.idle":"2023-04-19T00:03:31.389234Z","shell.execute_reply":"2023-04-19T00:03:31.388233Z","shell.execute_reply.started":"2023-04-19T00:03:30.690096Z"},"trusted":true},"outputs":[],"source":["cos = torch.nn.CosineSimilarity(dim=1)\n","\n","mediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\n","movements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\n","flavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:31.390923Z","iopub.status.busy":"2023-04-19T00:03:31.390552Z","iopub.status.idle":"2023-04-19T00:03:31.401925Z","shell.execute_reply":"2023-04-19T00:03:31.400890Z","shell.execute_reply.started":"2023-04-19T00:03:31.390885Z"},"trusted":true},"outputs":[],"source":["def interrogate(image: Image) -> str:\n","    caption = ci.generate_caption(image)\n","    image_features = ci.image_to_features(image)\n","    \n","    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n","    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n","    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n","\n","    if caption.startswith(medium):\n","        prompt = f\"{caption}, {movement}, {flaves}\"\n","    else:\n","        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n","\n","    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:31.409489Z","iopub.status.busy":"2023-04-19T00:03:31.409172Z","iopub.status.idle":"2023-04-19T00:03:36.166823Z","shell.execute_reply":"2023-04-19T00:03:36.165333Z","shell.execute_reply.started":"2023-04-19T00:03:31.409462Z"},"trusted":true},"outputs":[],"source":["prompts = []\n","\n","images_path = \"../input/stable-diffusion-image-to-prompts/images/\"\n","for image_name in images:\n","    img = Image.open(images_path + image_name).convert(\"RGB\")\n","\n","    generated = interrogate(img)\n","    \n","    prompts.append(generated)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.168648Z","iopub.status.busy":"2023-04-19T00:03:36.168262Z","iopub.status.idle":"2023-04-19T00:03:36.243001Z","shell.execute_reply":"2023-04-19T00:03:36.240796Z","shell.execute_reply.started":"2023-04-19T00:03:36.168609Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd4629e2e9424bc393e3231ae673f7e5","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["embeddings2 = st_model.encode(prompts).flatten()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.245181Z","iopub.status.busy":"2023-04-19T00:03:36.244573Z","iopub.status.idle":"2023-04-19T00:03:36.528768Z","shell.execute_reply":"2023-04-19T00:03:36.527597Z","shell.execute_reply.started":"2023-04-19T00:03:36.245143Z"},"trusted":true},"outputs":[{"data":{"text/plain":["64"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# FREE RESOURCES AS WE GO\n","del ci\n","del blip_model, clip_model\n","del st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.770635Z","iopub.status.busy":"2023-04-19T00:03:36.769892Z","iopub.status.idle":"2023-04-19T00:03:36.778235Z","shell.execute_reply":"2023-04-19T00:03:36.777270Z","shell.execute_reply.started":"2023-04-19T00:03:36.770586Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import timm\n","from sklearn.preprocessing import normalize"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.780368Z","iopub.status.busy":"2023-04-19T00:03:36.779521Z","iopub.status.idle":"2023-04-19T00:03:36.786979Z","shell.execute_reply":"2023-04-19T00:03:36.785745Z","shell.execute_reply.started":"2023-04-19T00:03:36.780329Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    model_path = '/kaggle/input/vit-large-patch16-224/vit_large_patch16_224.pth'\n","    model_name = 'vit_large_patch16_224'\n","    input_size = 224\n","    batch_size = 32"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.789010Z","iopub.status.busy":"2023-04-19T00:03:36.788634Z","iopub.status.idle":"2023-04-19T00:03:36.797139Z","shell.execute_reply":"2023-04-19T00:03:36.796105Z","shell.execute_reply.started":"2023-04-19T00:03:36.788977Z"},"trusted":true},"outputs":[],"source":["class DiffusionTestDataset(Dataset):\n","    def __init__(self, images, transform):\n","        self.images = images\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.images[idx])\n","        image = self.transform(image)\n","        return image"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.799431Z","iopub.status.busy":"2023-04-19T00:03:36.798918Z","iopub.status.idle":"2023-04-19T00:03:36.811079Z","shell.execute_reply":"2023-04-19T00:03:36.810056Z","shell.execute_reply.started":"2023-04-19T00:03:36.799396Z"},"trusted":true},"outputs":[],"source":["def predict(images, model_path, model_name, input_size, batch_size):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    transform = transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n","    \n","    dataset = DiffusionTestDataset(images, transform)\n","    dataloader = DataLoader(dataset=dataset, shuffle=False, batch_size=batch_size,\n","                            pin_memory=True, num_workers=2, drop_last=False)\n","\n","    model = timm.create_model(model_name, pretrained=False, num_classes=384)\n","    \n","    state_dict = torch.load(model_path)\n","    model.load_state_dict(state_dict)\n","    model.to(device)\n","    model.eval()\n","    \n","    preds = []\n","    for X in tqdm(dataloader, leave=False):\n","        X = X.to(device)\n","\n","        with torch.no_grad():\n","            X_out = model(X).cpu().numpy()\n","            X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)  # To avoid to overflow at normalize()\n","            X_out = normalize( X_out )\n","            preds.append(X_out)\n","    \n","    return np.vstack(preds).flatten()"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:03:36.813035Z","iopub.status.busy":"2023-04-19T00:03:36.812662Z","iopub.status.idle":"2023-04-19T00:03:53.768852Z","shell.execute_reply":"2023-04-19T00:03:53.767419Z","shell.execute_reply.started":"2023-04-19T00:03:36.812999Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n","imgIds = [i.stem for i in images]\n","EMBEDDING_LENGTH = 384\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n","\n","embeddings3 = predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:05:07.166614Z","iopub.status.busy":"2023-04-19T00:05:07.166241Z","iopub.status.idle":"2023-04-19T00:05:07.469605Z","shell.execute_reply":"2023-04-19T00:05:07.468462Z","shell.execute_reply.started":"2023-04-19T00:05:07.166581Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import random\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import timm\n","from timm.utils import AverageMeter\n","sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer\n","import warnings\n","warnings.filterwarnings('ignore')\n","from transformers import AutoModel, AutoProcessor\n","from sklearn.preprocessing import normalize\n","from pathlib import Path\n","\n","clip_processor = AutoProcessor.from_pretrained(\"/kaggle/input/openai-clip-pretrained\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class DiffusionDataset(Dataset):\n","    def __init__(self, df, transform, clip_processor=clip_processor):\n","        self.images = images\n","        self.transform = transform\n","        self.clip_processor = clip_processor\n","        \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.images[idx])\n","        image = self.transform(image)\n","        processed_image = self.clip_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n","        return processed_image \n","    \n","    \n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        clip = AutoModel.from_pretrained(\"/kaggle/input/openai-clip-pretrained\")\n","        self.vision = clip.vision_model\n","        self.fc = nn.Linear(1024, 384)\n","\n","    def forward(self, x):\n","        out = self.vision(x)['pooler_output']\n","        return self.fc(out)\n","\n","\n","def predict(images):\n","    transform = transforms.Compose([\n","        transforms.Resize(224),\n","        transforms.ToTensor()])\n","\n","    dataset = DiffusionDataset(images, transform)\n","    \n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        shuffle=False,\n","        batch_size=32,\n","        pin_memory=True,\n","        num_workers=2,\n","        drop_last=False)\n","   \n","    model = Net()\n","    sd = torch.load(\"/kaggle/input/openai-clip-ft/clip_vit_model5.pt\")\n","    model.load_state_dict(sd)\n","    #model = torch.compile(model)\n","    model.to(device)\n","    \n","    preds = []\n","    for X in tqdm(dataloader, leave=False):\n","        X = X.to(device)\n","\n","        with torch.no_grad():\n","            X_out = model(X).cpu().numpy()\n","            X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)  # To avoid to overflow at normalize()\n","            X_out = normalize( X_out )\n","            preds.append(X_out)\n","    \n","    return np.vstack(preds).flatten()"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:05:10.127522Z","iopub.status.busy":"2023-04-19T00:05:10.127150Z","iopub.status.idle":"2023-04-19T00:05:47.453579Z","shell.execute_reply":"2023-04-19T00:05:47.452187Z","shell.execute_reply.started":"2023-04-19T00:05:10.127488Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n","imgIds = [i.stem for i in images]\n","EMBEDDING_LENGTH = 384\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n","\n","embeddings4 = predict(images)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:05:47.461955Z","iopub.status.busy":"2023-04-19T00:05:47.459540Z","iopub.status.idle":"2023-04-19T00:05:47.469094Z","shell.execute_reply":"2023-04-19T00:05:47.468091Z","shell.execute_reply.started":"2023-04-19T00:05:47.461904Z"},"trusted":true},"outputs":[],"source":["embeddings = ratio_CLIP_Interrogator * embeddings2 + ratio_ViT_L_16 * embeddings3 + ratio_myclip * embeddings4"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:06:25.265917Z","iopub.status.busy":"2023-04-19T00:06:25.265425Z","iopub.status.idle":"2023-04-19T00:06:25.273536Z","shell.execute_reply":"2023-04-19T00:06:25.272472Z","shell.execute_reply.started":"2023-04-19T00:06:25.265832Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(\n","    index=imgId_eId,\n","    data=embeddings3,\n","    columns=['val']\n",").rename_axis('imgId_eId')"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-04-19T00:06:26.748782Z","iopub.status.busy":"2023-04-19T00:06:26.747836Z","iopub.status.idle":"2023-04-19T00:06:26.760454Z","shell.execute_reply":"2023-04-19T00:06:26.759122Z","shell.execute_reply.started":"2023-04-19T00:06:26.748734Z"},"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# BE SURE TO COMMENT OUT BEFORE TEST TIME"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["comp_path = Path('../input/stable-diffusion-image-to-prompts/')\n","sample_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\n","images = os.listdir(comp_path/\"images\")\n","image_ids = [i.split('.')[0] for i in images]\n","\n","EMBEDDING_LENGTH = 384\n","eIds = list(range(EMBEDDING_LENGTH))\n","\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(image_ids, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(image_ids)))]\n","\n","images = os.listdir(comp_path / 'images')\n","imgIds = [i.split('.')[0] for i in images]\n","EMBEDDING_LENGTH = 384\n","eIds = list(range(EMBEDDING_LENGTH))\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n","\n","\n","ground_truth = pd.read_csv('/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv')\n","ground_truth = pd.merge(pd.DataFrame(imgIds,columns=['imgId']),ground_truth,on='imgId',how='left')\n","st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')\n","\n","ground_truth_embeddings = st_model.encode(ground_truth.prompt).flatten()\n","\n","gte = pd.DataFrame(\n","                index=imgId_eId,\n","                data=ground_truth_embeddings,\n","                columns=['val']).rename_axis('imgId_eId')\n","\n","cv_prompts = pd.read_csv('/kaggle/working/submission.csv')\n","from scipy import spatial\n","vec1 = gte['val']\n","vec2 = cv_prompts['val']\n","cos_sim = 1 - spatial.distance.cosine(vec1, vec2)\n","print(cos_sim)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
