{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41e83e1",
   "metadata": {
    "papermill": {
     "duration": 0.003149,
     "end_time": "2023-04-10T13:42:30.520102",
     "exception": false,
     "start_time": "2023-04-10T13:42:30.516953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e64dca-63c9-4d76-8778-7ffeba3ff590",
   "metadata": {
    "papermill": {
     "duration": 3.349337,
     "end_time": "2023-04-10T13:42:46.620045",
     "exception": false,
     "start_time": "2023-04-10T13:42:43.270708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MUST USE GRADSCALER AND AUTOCAST OR TRAINING TAKES 3X AS LONG\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import spatial\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from timm.utils import AverageMeter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045f8194-720c-4ca0-8080-62b29c8ae1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIRM IMAGES EXIST\n",
    "#csv_file_path = \"hardcoded_prompts.csv\"\n",
    "#image_folder_path = \"hardcoded_images\"\n",
    "#df = pd.read_csv(csv_file_path)\n",
    "\n",
    "#def generate_image_path(index):\n",
    "#    image_name = f\"{index}.png\" \n",
    "#    return os.path.join(image_folder_path, image_name)\n",
    "\n",
    "#df['image_path'] = df.index.map(generate_image_path)\n",
    "\n",
    "#df.to_csv(\"hardcoded_prompts_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7380703e-c2bb-4d3b-9db5-b52107883de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def concatenate_csv_files(directory, output_filename):\n",
    "#    all_files = os.listdir(directory)\n",
    "#    csv_files = [file for file in all_files if file.endswith('.csv')]\n",
    "\n",
    "#    combined_data = pd.DataFrame()\n",
    "\n",
    "#    for file in csv_files:\n",
    "#        file_path = os.path.join(directory, file)\n",
    "#        data = pd.read_csv(file_path)\n",
    "#        combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "\n",
    "#    combined_data.to_csv(output_filename, index=False)\n",
    "    \n",
    "#directory = \"./prompt_artifacts\"\n",
    "#output_filename = \"prompt_artifacts_final.csv\"\n",
    "#concatenate_csv_files(directory, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efdca43-dc1d-49c1-bb5e-567f3b9e09b0",
   "metadata": {
    "papermill": {
     "duration": 3.349337,
     "end_time": "2023-04-10T13:42:46.620045",
     "exception": false,
     "start_time": "2023-04-10T13:42:43.270708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.18.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.18.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.18.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.18.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.18.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.18.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.18.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.18.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.18.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.18.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.18.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.18.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.18.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.18.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.18.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.18.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.19.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.19.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.19.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.19.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.19.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.19.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.19.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.19.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.19.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.19.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.19.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.19.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.19.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.19.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.19.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.19.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.20.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.20.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.20.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.20.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.20.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.20.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.20.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.20.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.21.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.21.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.21.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.21.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.21.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.21.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.21.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.21.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.22.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.22.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.22.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.22.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.22.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.22.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.22.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.22.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.23.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.23.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.23.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.23.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.23.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.23.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.23.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.23.layer_norm2.bias is set to be trainable.\n",
      "post_layernorm.weight is set to be trainable.\n",
      "post_layernorm.bias is set to be trainable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / trn/loss=0.4677, trn/cos=0.5323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / val/loss=0.4625, val/cos=0.5375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf2d184d1284141af17e20483fb1bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clip_processor = AutoProcessor.from_pretrained(\"clip-ft\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    model_name = 'clip-vit-large-patch14'\n",
    "    input_size = 224\n",
    "    batch_size = 128\n",
    "    num_epochs = 25\n",
    "    lr = 5e-4\n",
    "    seed = 21\n",
    "    unfreeze = 18 \n",
    "\n",
    "\n",
    "\n",
    "def get_dataloaders(trn_df, val_df, input_size, batch_size):\n",
    "    trn_dataset = DiffusionDataset(trn_df)\n",
    "    val_dataset = DiffusionDataset(val_df)\n",
    "    collator = DiffusionCollator()\n",
    "    \n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = DataLoader(\n",
    "        dataset=trn_dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=12,\n",
    "        drop_last=True,\n",
    "        collate_fn=collator)\n",
    "\n",
    "    dataloaders['val'] = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=12,\n",
    "        drop_last=False,\n",
    "        collate_fn=collator)\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(y_trues, y_preds):\n",
    "    return np.mean([\n",
    "        1 - spatial.distance.cosine(y_true, y_pred) \n",
    "        for y_true, y_pred in zip(y_trues, y_preds)])\n",
    "\n",
    "\n",
    "\n",
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, df, clip_processor=clip_processor):\n",
    "        self.df = df\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor()])\n",
    "        self.clip_processor = clip_processor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path'])\n",
    "        image = self.transform(image)\n",
    "        processed_image = self.clip_processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "        prompt = row['prompt']\n",
    "        return processed_image, prompt\n",
    "\n",
    "class DiffusionCollator:\n",
    "    def __init__(self):\n",
    "        self.st_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        images, prompts = zip(*batch)\n",
    "        images = torch.stack(images)\n",
    "        prompt_embeddings = self.st_model.encode(prompts, show_progress_bar=False, convert_to_tensor=True)\n",
    "        return images, prompt_embeddings\n",
    "    \n",
    "        \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        clip = AutoModel.from_pretrained(\"clip-ft\")\n",
    "        self.vision = clip.vision_model\n",
    "        self.fc = nn.Linear(1024, 384)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vision(x)['pooler_output']\n",
    "        return self.fc(out)\n",
    "    \n",
    "def load_pretrained_model():\n",
    "    model = Net()\n",
    "\n",
    "    trainable_model_weights = False\n",
    "    for name, child in model.named_children():\n",
    "        if name == 'vision':\n",
    "            for pn, p in child.named_parameters():\n",
    "                if str(CFG.unfreeze) in pn:\n",
    "                    \"\"\"start unfreezing layer to make weights trainable\"\"\"\n",
    "                    trainable_model_weights = True\n",
    "                p.requires_grad = trainable_model_weights\n",
    "                if p.requires_grad:\n",
    "                    print(f\"{pn} is set to be trainable.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train(trn_df, val_df, model_name, input_size, batch_size, num_epochs, lr):\n",
    "    dataloaders = get_dataloaders(trn_df, val_df, input_size, batch_size)\n",
    "\n",
    "    model = load_pretrained_model()\n",
    "    sd = torch.load(\"clip_vit_model5.pt\")\n",
    "    model.load_state_dict(sd)\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, fused=True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    criterion = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    best_score = -1.0\n",
    "    count = 0\n",
    "    run = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        run += 1\n",
    "        train_meters = {'loss': AverageMeter(), 'cos': AverageMeter()}\n",
    "        model.train()\n",
    "\n",
    "        for X, y in tqdm(dataloaders['train'], leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                X_out = model(X)\n",
    "                target = torch.ones(X.size(0)).to(device)\n",
    "                loss = criterion(X_out, y, target)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            trn_loss = loss.item()\n",
    "            trn_cos = cosine_similarity(X_out.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "            train_meters['loss'].update(trn_loss, n=X.size(0))\n",
    "            train_meters['cos'].update(trn_cos, n=X.size(0))\n",
    "\n",
    "        print(f'Epoch {epoch + 1} / trn/loss={train_meters[\"loss\"].avg:.4f}, trn/cos={train_meters[\"cos\"].avg:.4f}')\n",
    "\n",
    "        val_meters = {'loss': AverageMeter(), 'cos': AverageMeter()}\n",
    "        model.eval()\n",
    "\n",
    "        for X, y in tqdm(dataloaders['val'], leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    X_out = model(X)\n",
    "                    target = torch.ones(X.size(0)).to(device)\n",
    "                    loss = criterion(X_out, y, target)\n",
    "\n",
    "                val_loss = loss.item()\n",
    "                val_cos = cosine_similarity(X_out.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "\n",
    "            val_meters['loss'].update(val_loss, n=X.size(0))\n",
    "            val_meters['cos'].update(val_cos, n=X.size(0))\n",
    "\n",
    "        print(f'Epoch {epoch + 1} / val/loss={val_meters[\"loss\"].avg:.4f}, val/cos={val_meters[\"cos\"].avg:.4f}')\n",
    "\n",
    "        if val_meters['cos'].avg > best_score:\n",
    "            best_score = val_meters['cos'].avg\n",
    "            torch.save(model.state_dict(), f\"clip_vit_model{run}.pt\")\n",
    "            torch.save(optimizer.state_dict(), f\"clip_vit_opt{run}.pt\")\n",
    "        else:\n",
    "            count += 1\n",
    "            if count >= 3:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            \n",
    "\n",
    "df = pd.read_csv('filtered_image_data.csv') \n",
    "            \n",
    "trn_df, val_df = train_test_split(df, test_size=0.1, random_state=CFG.seed)\n",
    "\n",
    "train(trn_df, val_df, CFG.model_name, CFG.input_size, CFG.batch_size, CFG.num_epochs, CFG.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec07b14-9d59-48d7-8a19-3314abd20adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88882c7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4511c2-2f7d-48e7-ac2c-a55838daa8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-10T13:42:20.938135",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
